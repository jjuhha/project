{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.12.2-py3-none-any.whl (142 kB)\n",
      "     ---------------------------------------- 0.0/143.0 kB ? eta -:--:--\n",
      "     ------- ----------------------------- 30.7/143.0 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 143.0/143.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1262 sha256=60a765b8b37f3e9d4aa8260d10eca7625599c0ada5134aaf01a8863610fd89b8\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\25\\42\\45\\b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.2 bs4-0.0.1 soupsieve-2.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Collecting attrs>=20.1.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/61.2 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 61.2/61.2 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium)\n",
      "  Downloading cffi-1.16.0-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium)\n",
      "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "     ---------------------------------------- 0.0/118.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 118.7/118.7 kB 6.8 MB/s eta 0:00:00\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.4/10.0 MB 8.5 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.9/10.0 MB 11.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.3/10.0 MB 11.9 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.5/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.7/10.0 MB 8.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.2/10.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.6/10.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/10.0 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.5/10.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.0/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.5/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.9/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.4/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.9/10.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.3/10.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.8/10.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.2/10.0 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.0 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/10.0 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.5/10.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/10.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.4/10.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/10.0 MB 10.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "   ---------------------------------------- 0.0/461.6 kB ? eta -:--:--\n",
      "   --------------------------------------  460.8/461.6 kB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 461.6/461.6 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 181.6/181.6 kB 11.4 MB/s eta 0:00:00\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sortedcontainers, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium\n",
      "Successfully installed attrs-23.1.0 cffi-1.16.0 h11-0.14.0 outcome-1.3.0.post0 pycparser-2.21 pysocks-1.7.1 selenium-4.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from webdriver_manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver_manager)\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->webdriver_manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->webdriver_manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->webdriver_manager) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->webdriver_manager) (2023.11.17)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: python-dotenv, webdriver_manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver_manager-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxml\n",
      "  Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl.metadata (3.9 kB)\n",
      "Downloading lxml-4.9.3-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB 1.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.3/3.8 MB 5.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.8/3.8 MB 8.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.2/3.8 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 8.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.0/3.8 MB 9.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.5/3.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 10.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.5/3.8 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 10.1 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.9.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melon_album_list_2023-12-26_12.csv\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "포크/블루스\n",
      "마지막 페이지에 도달했습니다.\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "import os #makedirs()로 파일 생성하기\n",
    "import re #sub()로 특정 문자 변경하기\n",
    "import time #sleep()로 시간 텀 주기\n",
    "import requests #find_element()로 element 찾기\n",
    "from bs4 import BeautifulSoup #html 파싱하기\n",
    "import pandas as pd #dataframe 만들기\n",
    "from datetime import datetime #now()로 현재 시간 가져오기\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "url = \"https://www.melon.com/index.htm\"\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'\n",
    "\n",
    "\n",
    "def get_like_list() :\n",
    "    service = Service(executable_path = ChromeDriverManager().install())\n",
    "    browser = webdriver.Chrome(service = service)\n",
    "    \n",
    "    browser.implicitly_wait(5) #찾는 element가 생길 때까지 최대 5초 대기\n",
    "    browser.maximize_window() #브라우저 확대\n",
    "    \n",
    "    ##url 페이지로 이동\n",
    "    browser.get(url) #화면로딩이 안 끝났는데 다음 단계가 실행되면 에러나기 때문에 위에 implicitly wait\n",
    "    ##text만 뽑아낸 리스트를 만들기위해 빈 리스트 생성\n",
    "    title_list = [] \n",
    "    singer_list = []\n",
    "    album_list = []\n",
    "    like_list = []\n",
    "    genre_list = []\n",
    "    release_list = []\n",
    "    ##버튼 찾기 및 클릭\n",
    "    login_btn = browser.find_element(By.CSS_SELECTOR,\"#gnbLoginDiv > div > button > span\")\n",
    "    login_btn.click()\n",
    "    time.sleep(2) #클릭하고 시간 좀 주기\n",
    "    login2_btn = browser.find_element(By.CSS_SELECTOR,\"#conts_section > div > div > div:nth-child(2) > button > span\")\n",
    "    login2_btn.click()\n",
    "    time.sleep(1) #클릭하고 시간 좀 주기\n",
    "\n",
    "    windows = browser.window_handles\n",
    "    browser.switch_to.window(windows[-1])  # 가장 최근에 열린 창으로 전환\n",
    "\n",
    "    # 탭 키를 누르고 이메일 입력\n",
    "    body = browser.find_element(By.CSS_SELECTOR,\"#loginId--1\")\n",
    "    # body.send_keys(Keys.TAB)  # 탭 키 누르기\n",
    "    body.send_keys(\"*****@naver.com\")  # 이메일 입력\n",
    "\n",
    "    body = browser.find_element(By.CSS_SELECTOR,\"#password--2\")\n",
    "    body.send_keys(\"******\")  # 비밀번호 입력\n",
    "\n",
    "    login3_btn = browser.find_element(By.CSS_SELECTOR,'#mainContent > div > div > form > div.confirm_btn > button.btn_g.highlight.submit')\n",
    "    login3_btn.click()\n",
    "    time.sleep(2) #클릭하고 시간 좀 주기\n",
    "    \n",
    "    windows = browser.window_handles\n",
    "    browser.switch_to.window(windows[-1])  # 가장 최근에 열린 창으로 전환\n",
    "\n",
    "    mymusic_btn = browser.find_element(By.CSS_SELECTOR,'#gnb_menu > ul.sub_gnb > li > a > span.menu_bg.menu09')\n",
    "    mymusic_btn.click() #클릭    \n",
    "    time.sleep(1)\n",
    "    like_btn = browser.find_element(By.CSS_SELECTOR,'#conts > div.wrap_tab03.type09 > ul > li:nth-child(2) > a > span')\n",
    "    like_btn.click() #클릭  \n",
    "    time.sleep(1)\n",
    "    \n",
    "    current_page = 1  # 현재 페이지 번호\n",
    "    last_page = 110  # 마지막 페이지 번호, 실제 마지막 페이지 번호로 설정 필요 # 9 페이지 안됨\n",
    "\n",
    "    while current_page <= last_page:\n",
    "\n",
    "    #소스 가져와서 필요한 text 뽑아오기\n",
    "        html = browser.page_source #해당 페이지 소스 불러와서 html에 저장\n",
    "        soup = BeautifulSoup(html, \"html.parser\") #html을 파싱하여 soup에 저장\n",
    "        title = soup.select(\"td:nth-child(3) > div > div\") #soup에서 해당 태그의 element들을 리스트로 반환\n",
    "        singer = soup.select(\"td:nth-child(4) > div > div.ellipsis > a:nth-child(1)\") \n",
    "        album = soup.select('td:nth-child(5) > div > div > a')\n",
    "        like = soup.select('div > button > span.cnt')\n",
    "        # 버튼을 찾습니다.\n",
    "        album_btns = browser.find_elements(By.CSS_SELECTOR, 'td:nth-child(5) > div > div > a')\n",
    "\n",
    "        for i in range(20):\n",
    "            album_btns[i].click() # 앨범 페이지로 이동\n",
    "            time.sleep(1) # 페이지 로딩 대기\n",
    "\n",
    "            # 새로운 페이지의 정보를 가져옵니다.\n",
    "            new_html = browser.page_source\n",
    "            new_soup = BeautifulSoup(new_html, \"html.parser\")\n",
    "            new_genre = new_soup.select('div.meta > dl > dd:nth-child(4)')\n",
    "            new_year = new_soup.select('div.meta > dl > dd:nth-child(2)')\n",
    "            # 장르 정보 리스트에 추가합니다.\n",
    "            genre_list.append(new_genre[0].get_text().strip())\n",
    "            release_list.append(new_year[0].get_text().strip())\n",
    "            browser.back() # 이전 페이지로 돌아갑니다.\n",
    "            time.sleep(1)\n",
    "            \n",
    "        print(genre)\n",
    "        for i in range(len(title)) : #가져온 element 리스트의 길이만큼 \n",
    "            title_list.append(title[i].get_text().strip()) #제목, 가수, 내용 text를 각각의 리스트에 어펜드\n",
    "            singer_list.append(singer[i].get_text().strip())\n",
    "            album_list.append(album[i].get_text().strip()) \n",
    "            like_list.append(like[i].get_text().strip())\n",
    "            # genre_list.append(genre[i].get_text().strip()) \n",
    "\n",
    "        # 다음 페이지로 넘어가기\n",
    "        if current_page < last_page:\n",
    "            next_page_number = (current_page * 20) + 1  # 다음 페이지 번호 계산 (예: 21, 41, 61, ...)\n",
    "            browser.execute_script(f\"pageObj.sendPage('{next_page_number}');\")\n",
    "            time.sleep(2)  # 페이지 로딩 대기\n",
    "            current_page += 1\n",
    "        else:\n",
    "            print(\"마지막 페이지에 도달했습니다.\")\n",
    "            break\n",
    "            \n",
    "    browser.close() #브라우저 끄기\n",
    "\n",
    "    return pd.DataFrame({ #데이터프레임 만들어서 반환해주기\n",
    "            \"title\" : title_list,\n",
    "            \"singer\" : singer_list,\n",
    "            \"album\" : album_list,\n",
    "            \"genre\" : genre_list,\n",
    "            \"release\" : release_list,\n",
    "            \"like\" : like_list\n",
    "    })\n",
    "\n",
    "#노래 목록 저장하기 \n",
    "if __name__ == \"__main__\" :\n",
    "    dt = datetime.now().strftime(\"%Y-%m-%d\") #strftime() : 날짜시간을 원하는 형태의 문자열로 변환\n",
    "    file_path = f\"melon_album_list_{dt}_12.csv\"\n",
    "    print(file_path)\n",
    "    result = get_like_list()\n",
    "    result.to_csv(file_path, index=False)\n",
    "###파일 저장하려는 디렉토리 만들기\n",
    "    save_path = f\"melon/{dt}\"\n",
    "    os.makedirs(save_path, exist_ok=True) #상위 디렉토리까지 생성 #news디렉토리 아래 디렉토리 만들어라. news디렉토리 없으면 새로 만들어줌. \n",
    "###데이터프레임의 열을 각각 리스트로 가져오기    \n",
    "    titles = result[\"title\"] \n",
    "    singers = result[\"singer\"]\n",
    "    albums = result[\"album\"]\n",
    "    genres = result[\"genre\"]\n",
    "    releases = result[\"release\"]\n",
    "    likes = result['like']\n",
    "###각 리스트를 for문 돌려서 제목,가수,내용 합쳐서 파일 만들기\n",
    "    for title, singer, album, genre in zip(titles, singers, albums, genres) : \n",
    "        title = re.sub('[^\\w]','',title)\n",
    "        with open(f\"{save_path}/{title}-{singer}.txt\",\"wt\", encoding = 'utf-8') as fw : #묶어서 저장하겠다. #파일제목\n",
    "            fw.write(album) #파일내용\n",
    "    print(\"완료\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
